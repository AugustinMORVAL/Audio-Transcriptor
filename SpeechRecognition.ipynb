{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "Google Speech Recognition could not understand audio\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Create a recognizer object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Use the microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    # Listen for audio input\n",
    "    audio = r.listen(source)\n",
    "\n",
    "try:\n",
    "    # Use Google's speech recognition to convert audio to text\n",
    "    text = r.recognize_google(audio)\n",
    "    # Write the transcription to a file\n",
    "    with open(\"transcription.txt\", \"w\") as file:\n",
    "        file.write(text)\n",
    "    print(\"Transcription saved to transcription.txt\")\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "Google Speech Recognition could not understand audio.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "\n",
    "# Create a recognizer object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Use the microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    # Listen for audio input\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# Check if audio data is received\n",
    "if audio:\n",
    "    # Convert audio data to numpy array\n",
    "    audio_data = np.frombuffer(audio.frame_data, np.int16)\n",
    "\n",
    "    # Perform noise reduction\n",
    "    reduced_noise = nr.reduce_noise(y=audio_data, sr=audio.sample_rate)\n",
    "\n",
    "    # Convert numpy array back to audio data\n",
    "    reduced_noise_audio = sr.AudioData(reduced_noise.tobytes(), sample_rate=audio.sample_rate, sample_width=audio.sample_width)\n",
    "\n",
    "    try:\n",
    "        # Use Google's speech recognition to convert audio to text\n",
    "        text = r.recognize_google(reduced_noise_audio, language=\"fr-FR\")\n",
    "        print(\"Transcript:\", text)\n",
    "        # Write the transcript to a file\n",
    "        with open(\"transcript.txt\", \"a\") as file:\n",
    "            file.write(text + \"\\n\")\n",
    "            print('transcript updated')\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand audio.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "else:\n",
    "    print(\"No audio data received.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "\n",
      "Could not download 'None' pipeline.\n",
      "It might be because the pipeline is private or gated so make\n",
      "sure to authenticate. Visit https://hf.co/settings/tokens to\n",
      "create your access token and retry with:\n",
      "\n",
      "   >>> Pipeline.from_pretrained('None',\n",
      "   ...                          use_auth_token=YOUR_AUTH_TOKEN)\n",
      "\n",
      "If this still does not work, it might be because the pipeline is gated:\n",
      "visit https://hf.co/None to accept the user conditions.\n",
      "\n",
      "Could not download 'None' model.\n",
      "It might be because the model is private or gated so make\n",
      "sure to authenticate. Visit https://hf.co/settings/tokens to\n",
      "create your access token and retry with:\n",
      "\n",
      "   >>> Model.from_pretrained('None',\n",
      "   ...                       use_auth_token=YOUR_AUTH_TOKEN)\n",
      "\n",
      "If this still does not work, it might be because the model is gated:\n",
      "visit https://hf.co/None to accept the user conditions.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Initialize the pipeline\u001b[39;00m\n\u001b[0;32m     37\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m                                     use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_rwIQibbtsVOdUWlxnFNUTUtUJHJnszJflz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m inference \u001b[38;5;241m=\u001b[39m \u001b[43mInference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Apply the pipeline to the temporary audio file\u001b[39;00m\n\u001b[0;32m     42\u001b[0m diarization \u001b[38;5;241m=\u001b[39m inference(temp_filename)\n",
      "File \u001b[1;32mc:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\inference.py:110\u001b[0m, in \u001b[0;36mInference.__init__\u001b[1;34m(self, model, window, duration, step, pre_aggregation_hook, skip_aggregation, skip_conversion, device, batch_size, use_auth_token)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m     model\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Model)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline, Inference\n",
    "import speech_recognition as sr\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "# Create a recognizer object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Set the pause threshold to 1.0 seconds\n",
    "r.pause_threshold = 1.0\n",
    "\n",
    "# Use the microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    # Listen for audio input\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# Check if audio data is received\n",
    "if audio:\n",
    "    # Convert audio data to numpy array\n",
    "    audio_data = np.frombuffer(audio.frame_data, np.int16)\n",
    "\n",
    "    # Perform noise reduction\n",
    "    reduced_noise = nr.reduce_noise(y=audio_data.flatten(), sr=audio.sample_rate)\n",
    "\n",
    "    # Convert numpy array back to audio data\n",
    "    reduced_noise_audio = sr.AudioData(reduced_noise.tobytes(), sample_rate=audio.sample_rate, sample_width=audio.sample_width)\n",
    "\n",
    "    # Save the reduced noise audio to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp:\n",
    "        temp_filename = temp.name\n",
    "        with open(temp_filename, \"wb\") as f:\n",
    "            f.write(reduced_noise_audio.get_wav_data())\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\"None\",\n",
    "                                        use_auth_token=\"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\")\n",
    "    inference = Inference(pipeline)\n",
    "\n",
    "    # Apply the pipeline to the temporary audio file\n",
    "    diarization = inference(temp_filename)\n",
    "\n",
    "    # Print the diarization output\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n"
     ]
    },
    {
     "ename": "UnknownValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Listen for audio input\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     audio \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mlisten(source)\n\u001b[1;32m---> 13\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfr-FR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Save the audio data to a file\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:263\u001b[0m, in \u001b[0;36mrecognize_legacy\u001b[1;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[0;32m    256\u001b[0m response_text \u001b[38;5;241m=\u001b[39m obtain_transcription(\n\u001b[0;32m    257\u001b[0m     request, timeout\u001b[38;5;241m=\u001b[39mrecognizer\u001b[38;5;241m.\u001b[39moperation_timeout\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    260\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m OutputParser(\n\u001b[0;32m    261\u001b[0m     show_all\u001b[38;5;241m=\u001b[39mshow_all, with_confidence\u001b[38;5;241m=\u001b[39mwith_confidence\n\u001b[0;32m    262\u001b[0m )\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:134\u001b[0m, in \u001b[0;36mOutputParser.parse\u001b[1;34m(self, response_text)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, response_text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 134\u001b[0m     actual_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_all:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m actual_result\n",
      "File \u001b[1;32mc:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:183\u001b[0m, in \u001b[0;36mOutputParser.convert_to_result\u001b[1;34m(response_text)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m UnknownValueError()\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnknownValueError()\n",
      "\u001b[1;31mUnknownValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# Create a recognizer object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Use the microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    # Listen for audio input\n",
    "    audio = r.listen(source)\n",
    "    text = r.recognize_google(audio, language= \"fr-FR\")\n",
    "\n",
    "# Save the audio data to a file\n",
    "with open(\"audio.wav\", \"wb\") as f:\n",
    "    f.write(audio.get_wav_data())\n",
    "\n",
    "# Convert audio file to wav format\n",
    "sound = AudioSegment.from_wav(\"audio.wav\")\n",
    "sound.export(\"audio.wav\", format=\"wav\")\n",
    "\n",
    "# Load pretrained pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                    use_auth_token='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')\n",
    "\n",
    "# Process audio file\n",
    "output = pipeline(\"audio.wav\")\n",
    "print(\"Diarization:\\n\", output)\n",
    "print(\"\\nTranscription:\\n\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File meeting-clip2.wav does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m diarization_pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyannote/speaker-diarization-3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_rwIQibbtsVOdUWlxnFNUTUtUJHJnszJflz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Process audio file for diarization\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m diarization \u001b[38;5;241m=\u001b[39m \u001b[43mdiarization_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize recognizer class for speech recognition\u001b[39;00m\n\u001b[0;32m     15\u001b[0m recognizer \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n",
      "File \u001b[1;32mc:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:321\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, file, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA pipeline must be instantiated with `pipeline.instantiate(paramaters)` before it can be applied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to use parameters provided by `pipeline.default_parameters()` but those are not compatible. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    315\u001b[0m         )\n\u001b[0;32m    317\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline has been automatically instantiated with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefault_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m     )\n\u001b[1;32m--> 321\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mAudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    324\u001b[0m     file \u001b[38;5;241m=\u001b[39m ProtocolFile(file, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors)\n",
      "File \u001b[1;32mc:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:171\u001b[0m, in \u001b[0;36mAudio.validate_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    169\u001b[0m     path \u001b[38;5;241m=\u001b[39m Path(file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m     file\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m.\u001b[39mstem)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: File meeting-clip2.wav does not exist"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyannote.audio import Pipeline\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Define the audio file path\n",
    "audio_file = \"meeting-clip2.wav\"\n",
    "\n",
    "# Load pretrained pipeline for speaker diarization\n",
    "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token='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')\n",
    "\n",
    "# Process audio file for diarization\n",
    "diarization = diarization_pipeline(audio_file)\n",
    "\n",
    "# Initialize recognizer class for speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Open the audio file\n",
    "with sr.AudioFile(audio_file) as source:\n",
    "    # Read the entire audio file\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "    # Use the recognizer to convert audio to text\n",
    "    transcription = recognizer.recognize_google(audio_data, language=\"en-EN\")\n",
    "\n",
    "# Print the diarization and transcription results\n",
    "print(\"Diarization:\\n\", diarization)\n",
    "print(\"\\nTranscription:\\n\", transcription)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization:\n",
      " [ 00:00:00.030 -->  00:00:00.182] A SPEAKER_00\n",
      "[ 00:00:01.195 -->  00:00:04.992] B SPEAKER_00\n",
      "[ 00:00:05.565 -->  00:00:10.442] C SPEAKER_00\n",
      "[ 00:00:10.999 -->  00:00:16.652] D SPEAKER_00\n",
      "[ 00:00:17.614 -->  00:00:19.420] E SPEAKER_01\n",
      "\n",
      "Transcription:\n",
      " well from my point of view what Paul is proposing sounds fine I am a bit concerned about working with the system of core hours and then flexible hours but I think we all need time to read through Paul's proposal in more detail before discussing it any further make sure that sounds reasonable\n",
      "\n",
      "Detected language: en-US\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import speech_recognition as sr\n",
    "from langdetect import detect\n",
    "\n",
    "# Parameters to select \n",
    "audio_file = \"audio-test\\meeting-clip2.wav\"\n",
    "audio_lang = \"\"\n",
    "\n",
    "# Pre-set\n",
    "languages = {\"french\": \"fr-FR\", \"english\": \"en-US\", \"spanish\": \"es-ES\", \"german\": \"de-DE\"}\n",
    "language =  languages.get(audio_lang, \"en-US\")  # default to English if language not found\n",
    "\n",
    "# Load pretrained pipeline for speaker diarization\n",
    "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token='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')\n",
    "\n",
    "# Process audio file for diarization\n",
    "diarization = diarization_pipeline(audio_file)\n",
    "\n",
    "# Initialize recognizer class for speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Open the audio file\n",
    "with sr.AudioFile(audio_file) as source:\n",
    "    # Read the entire audio file\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "    # Use the recognizer to convert audio to text\n",
    "    transcription = recognizer.recognize_google(audio_data, language=language)\n",
    "\n",
    "print(\"Diarization:\\n\", diarization)\n",
    "print(\"\\nTranscription:\\n\", transcription)\n",
    "print(\"\\nDetected language:\", language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
